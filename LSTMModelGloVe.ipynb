{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import datetime\n",
    "from math import floor\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device('cuda')\n",
    "iterator_device = 0\n",
    "##uncomment for cpu\n",
    "#device = torch.device('cpu')\n",
    "#iterator_device = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "blurb_field = data.Field(sequential=True, use_vocab=True, lower=True, tokenize=\"spacy\", include_lengths=True)\n",
    "state_field = data.LabelField(sequential=False, use_vocab=False, tensor_type=torch.FloatTensor,\n",
    "                              preprocessing=lambda x:1 if x=='successful' else 0)\n",
    "dataset = data.TabularDataset(path='df_text_eng.csv',format='csv',skip_header=True,fields=[('Unnamed: 0', None),('blurb', blurb_field),('state', state_field)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size:  150859\n",
      "Test Set Size:  32327\n",
      "Validation Set Size:  32327\n"
     ]
    }
   ],
   "source": [
    "train, test, validation = dataset.split(random_state=random.seed(seed), split_ratio=[70,15,15])\n",
    "print(\"Training Set Size: \", len(train))\n",
    "print(\"Test Set Size: \", len(test))\n",
    "print(\"Validation Set Size: \", len(validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size used:  28760\n"
     ]
    }
   ],
   "source": [
    "##randomly\n",
    "#words that appear less than 3 times (2 or less) will be considered unknown words with tag \"<unk>\", \n",
    "#they will have the same word embedding\n",
    "#vocabulary of the training set will only be used to emulate real world situtaions when the test set is unknown\n",
    "blurb_field.build_vocab(train,min_freq=3,vectors=\"glove.6B.100d\")\n",
    "print(\"Vocabulary size used: \",len(blurb_field.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_iter = data.BucketIterator(dataset=train, batch_size=batch_size, sort_key=lambda x:len(x.blurb),device=iterator_device,\n",
    "                                repeat=False, train=True, sort_within_batch=True)\n",
    "test_iter = data.BucketIterator(dataset=test, batch_size=batch_size, sort_key=lambda x:len(x.blurb),device=iterator_device,\n",
    "                                train=False, sort_within_batch=True)\n",
    "validation_iter = data.BucketIterator(dataset=validation, batch_size=batch_size, sort_key=lambda x:len(x.blurb),device=iterator_device,\n",
    "                                      train=False, sort_within_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size, padding_idx=None, init_embedding=None):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        if (init_embedding is not None):\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=padding_idx, _weight=init_embedding)    # word embedding\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=padding_idx)    # word embedding\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size)  #LSTM layer, default activation function is tanh\n",
    "        self.out = nn.Linear(hidden_size, output_size)   # output layer (Fully Connected)\n",
    "        \n",
    "\n",
    "    def forward(self, x, x_lengths):\n",
    "        #x -> [sentence length(max, smaller sentences are padded), minibatch size]\n",
    "        embeds = self.embedding(x)\n",
    "        \n",
    "        #embeds -> [sentence length, minibatch size, embedding size]\n",
    "        embeds = nn.utils.rnn.pack_padded_sequence(embeds,x_lengths)\n",
    "        all_hiddens, last_hidden = self.lstm(embeds)      # activation function for hidden layer\n",
    "        #the first dimension of the hidden layer is 1 (we use one hidden layer uniderectional LSTMs), squeeze removes it to be \n",
    "        #able to enter the fully connected layer with dimensions [batch size, hidden size]\n",
    "        score = self.out(last_hidden.squeeze(0))           \n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(predictions, y):\n",
    "    predictions = torch.round(torch.sigmoid(predictions))\n",
    "    correct = (sum(predictions == y)).float()\n",
    "    accuracy = correct/len(predictions)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_idx = blurb_field.vocab.stoi['<pad>']\n",
    "embed_size = 100\n",
    "hidden_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmmodel = LSTMNet(len(blurb_field.vocab),embed_size,hidden_size,1, padding_idx=padding_idx, init_embedding=blurb_field.vocab.vectors)\n",
    "optimizer = optim.SGD(lstmmodel.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "lstmmodel = lstmmodel.to(device)\n",
    "loss_fn = loss_fn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, loss_fn):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for minibatch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(minibatch.blurb[0],minibatch.blurb[1]).squeeze(1)\n",
    "        loss = loss_fn(predictions, minibatch.state)\n",
    "        accuracy = calc_accuracy(predictions, minibatch.state)    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_accuracy += accuracy.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_accuracy / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, loss_fn):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for minibatch in iterator:\n",
    "            predictions = model(minibatch.blurb[0],minibatch.blurb[1]).squeeze(1)\n",
    "            loss = loss_fn(predictions, minibatch.state)\n",
    "            accuracy = calc_accuracy(predictions, minibatch.state)    \n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_accuracy += accuracy.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_accuracy / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchtext/data/field.py:321: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train), lengths\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 0.6932, Train Accuracy: 50.61%, Validation Loss: 0.6920, Validation Accuracy: 52.60%\n",
      "Total Time Passed: 0 hours, 0 minutes, 22.08 seconds\n",
      "Epoch: 02, Train Loss: 0.6915, Train Accuracy: 52.76%, Validation Loss: 0.6904, Validation Accuracy: 54.45%\n",
      "Total Time Passed: 0 hours, 0 minutes, 44.10 seconds\n",
      "Epoch: 03, Train Loss: 0.6901, Train Accuracy: 54.15%, Validation Loss: 0.6889, Validation Accuracy: 54.98%\n",
      "Total Time Passed: 0 hours, 1 minutes, 6.10 seconds\n",
      "Epoch: 04, Train Loss: 0.6889, Train Accuracy: 54.82%, Validation Loss: 0.6877, Validation Accuracy: 55.82%\n",
      "Total Time Passed: 0 hours, 1 minutes, 28.00 seconds\n",
      "Epoch: 05, Train Loss: 0.6878, Train Accuracy: 55.41%, Validation Loss: 0.6865, Validation Accuracy: 56.16%\n",
      "Total Time Passed: 0 hours, 1 minutes, 49.93 seconds\n",
      "Epoch: 06, Train Loss: 0.6868, Train Accuracy: 55.67%, Validation Loss: 0.6854, Validation Accuracy: 56.46%\n",
      "Total Time Passed: 0 hours, 2 minutes, 11.77 seconds\n",
      "Epoch: 07, Train Loss: 0.6858, Train Accuracy: 55.94%, Validation Loss: 0.6844, Validation Accuracy: 56.65%\n",
      "Total Time Passed: 0 hours, 2 minutes, 33.87 seconds\n",
      "Epoch: 08, Train Loss: 0.6849, Train Accuracy: 55.97%, Validation Loss: 0.6835, Validation Accuracy: 56.68%\n",
      "Total Time Passed: 0 hours, 2 minutes, 55.46 seconds\n",
      "Epoch: 09, Train Loss: 0.6841, Train Accuracy: 56.07%, Validation Loss: 0.6826, Validation Accuracy: 56.69%\n",
      "Total Time Passed: 0 hours, 3 minutes, 17.10 seconds\n",
      "Epoch: 10, Train Loss: 0.6833, Train Accuracy: 56.17%, Validation Loss: 0.6818, Validation Accuracy: 56.84%\n",
      "Total Time Passed: 0 hours, 3 minutes, 38.80 seconds\n",
      "Epoch: 11, Train Loss: 0.6826, Train Accuracy: 56.24%, Validation Loss: 0.6811, Validation Accuracy: 56.85%\n",
      "Total Time Passed: 0 hours, 4 minutes, 0.47 seconds\n",
      "Epoch: 12, Train Loss: 0.6819, Train Accuracy: 56.38%, Validation Loss: 0.6804, Validation Accuracy: 56.99%\n",
      "Total Time Passed: 0 hours, 4 minutes, 21.99 seconds\n",
      "Epoch: 13, Train Loss: 0.6814, Train Accuracy: 56.36%, Validation Loss: 0.6798, Validation Accuracy: 57.20%\n",
      "Total Time Passed: 0 hours, 4 minutes, 43.50 seconds\n",
      "Epoch: 14, Train Loss: 0.6808, Train Accuracy: 56.45%, Validation Loss: 0.6793, Validation Accuracy: 57.05%\n",
      "Total Time Passed: 0 hours, 5 minutes, 5.10 seconds\n",
      "Epoch: 15, Train Loss: 0.6803, Train Accuracy: 56.47%, Validation Loss: 0.6787, Validation Accuracy: 57.20%\n",
      "Total Time Passed: 0 hours, 5 minutes, 26.17 seconds\n",
      "Epoch: 16, Train Loss: 0.6798, Train Accuracy: 56.58%, Validation Loss: 0.6783, Validation Accuracy: 57.20%\n",
      "Total Time Passed: 0 hours, 5 minutes, 47.05 seconds\n",
      "Epoch: 17, Train Loss: 0.6794, Train Accuracy: 56.66%, Validation Loss: 0.6779, Validation Accuracy: 57.24%\n",
      "Total Time Passed: 0 hours, 6 minutes, 7.91 seconds\n",
      "Epoch: 18, Train Loss: 0.6790, Train Accuracy: 56.70%, Validation Loss: 0.6774, Validation Accuracy: 57.24%\n",
      "Total Time Passed: 0 hours, 6 minutes, 28.82 seconds\n",
      "Epoch: 19, Train Loss: 0.6787, Train Accuracy: 56.80%, Validation Loss: 0.6770, Validation Accuracy: 57.27%\n",
      "Total Time Passed: 0 hours, 6 minutes, 49.68 seconds\n",
      "Epoch: 20, Train Loss: 0.6783, Train Accuracy: 56.85%, Validation Loss: 0.6766, Validation Accuracy: 57.31%\n",
      "Total Time Passed: 0 hours, 7 minutes, 10.41 seconds\n",
      "Epoch: 21, Train Loss: 0.6780, Train Accuracy: 56.94%, Validation Loss: 0.6763, Validation Accuracy: 57.28%\n",
      "Total Time Passed: 0 hours, 7 minutes, 31.34 seconds\n",
      "Epoch: 22, Train Loss: 0.6776, Train Accuracy: 57.00%, Validation Loss: 0.6759, Validation Accuracy: 57.40%\n",
      "Total Time Passed: 0 hours, 7 minutes, 52.22 seconds\n",
      "Epoch: 23, Train Loss: 0.6773, Train Accuracy: 57.04%, Validation Loss: 0.6756, Validation Accuracy: 57.42%\n",
      "Total Time Passed: 0 hours, 8 minutes, 13.07 seconds\n",
      "Epoch: 24, Train Loss: 0.6770, Train Accuracy: 57.09%, Validation Loss: 0.6752, Validation Accuracy: 57.50%\n",
      "Total Time Passed: 0 hours, 8 minutes, 33.96 seconds\n",
      "Epoch: 25, Train Loss: 0.6767, Train Accuracy: 57.12%, Validation Loss: 0.6750, Validation Accuracy: 57.63%\n",
      "Total Time Passed: 0 hours, 8 minutes, 54.87 seconds\n",
      "Epoch: 26, Train Loss: 0.6763, Train Accuracy: 57.26%, Validation Loss: 0.6746, Validation Accuracy: 57.66%\n",
      "Total Time Passed: 0 hours, 9 minutes, 15.69 seconds\n",
      "Epoch: 27, Train Loss: 0.6760, Train Accuracy: 57.31%, Validation Loss: 0.6742, Validation Accuracy: 57.69%\n",
      "Total Time Passed: 0 hours, 9 minutes, 36.57 seconds\n",
      "Epoch: 28, Train Loss: 0.6757, Train Accuracy: 57.35%, Validation Loss: 0.6738, Validation Accuracy: 57.77%\n",
      "Total Time Passed: 0 hours, 9 minutes, 57.44 seconds\n"
     ]
    }
   ],
   "source": [
    "no_epochs = 100\n",
    "#import ipdb; ipdb.set_trace() # debugging starts here\n",
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "init_time = datetime.datetime.now()\n",
    "for epoch in range(no_epochs):\n",
    "    train_loss, train_accuracy = train(lstmmodel, train_iter, optimizer, loss_fn)\n",
    "    validation_loss, validation_accuracy = evaluate(lstmmodel, validation_iter, loss_fn)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    validation_accuracies.append(validation_accuracy)\n",
    "    current_time = datetime.datetime.now()\n",
    "    total_time = (current_time-init_time).total_seconds()\n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy*100:.2f}%, ' +  \n",
    "          f'Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_accuracy*100:.2f}%')\n",
    "    print(f'Total Time Passed: {floor(total_time/3600)} hours, {floor(total_time/60)%60} minutes, {total_time%60:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = evaluate(lstmmodel, test_iter, loss_fn)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1,no_epochs+1),train_accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LSTMModelGloveTraining.txt', 'w') as f:\n",
    "    for item in train_accuracies:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LSTMModelGloveVal.txt', 'w') as f:\n",
    "    for item in validation_accuracies:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
