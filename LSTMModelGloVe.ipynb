{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import datetime\n",
    "from math import floor\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device('cuda')\n",
    "iterator_device = 0\n",
    "##uncomment for cpu\n",
    "#device = torch.device('cpu')\n",
    "#iterator_device = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "blurb_field = data.Field(sequential=True, use_vocab=True, lower=True, tokenize=\"spacy\", include_lengths=True)\n",
    "state_field = data.LabelField(sequential=False, use_vocab=False, tensor_type=torch.FloatTensor,\n",
    "                              preprocessing=lambda x:1 if x=='successful' else 0)\n",
    "dataset = data.TabularDataset(path='df_text_eng.csv',format='csv',skip_header=True,fields=[('Unnamed: 0', None),('blurb', blurb_field),('state', state_field)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size:  150859\n",
      "Test Set Size:  32327\n",
      "Validation Set Size:  32327\n"
     ]
    }
   ],
   "source": [
    "train, test, validation = dataset.split(random_state=random.seed(seed), split_ratio=[70,15,15])\n",
    "print(\"Training Set Size: \", len(train))\n",
    "print(\"Test Set Size: \", len(test))\n",
    "print(\"Validation Set Size: \", len(validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size used:  28760\n"
     ]
    }
   ],
   "source": [
    "##randomly\n",
    "#words that appear less than 3 times (2 or less) will be considered unknown words with tag \"<unk>\", \n",
    "#they will have the same word embedding\n",
    "#vocabulary of the training set will only be used to emulate real world situtaions when the test set is unknown\n",
    "blurb_field.build_vocab(train,min_freq=3,vectors=\"glove.6B.100d\")\n",
    "print(\"Vocabulary size used: \",len(blurb_field.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_iter = data.BucketIterator(dataset=train, batch_size=batch_size, sort_key=lambda x:len(x.blurb),device=iterator_device,\n",
    "                                repeat=False, train=True, sort_within_batch=True)\n",
    "test_iter = data.BucketIterator(dataset=test, batch_size=batch_size, sort_key=lambda x:len(x.blurb),device=iterator_device,\n",
    "                                train=False, sort_within_batch=True)\n",
    "validation_iter = data.BucketIterator(dataset=validation, batch_size=batch_size, sort_key=lambda x:len(x.blurb),device=iterator_device,\n",
    "                                      train=False, sort_within_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size, padding_idx=None, init_embedding=None):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        if (init_embedding is not None):\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=padding_idx, _weight=init_embedding)    # word embedding\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=padding_idx)    # word embedding\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size)  #LSTM layer, default activation function is tanh\n",
    "        self.out = nn.Linear(hidden_size, output_size)   # output layer (Fully Connected)\n",
    "        \n",
    "\n",
    "    def forward(self, x, x_lengths):\n",
    "        #x -> [sentence length(max, smaller sentences are padded), minibatch size]\n",
    "        embeds = self.embedding(x)\n",
    "        \n",
    "        #embeds -> [sentence length, minibatch size, embedding size]\n",
    "        embeds = nn.utils.rnn.pack_padded_sequence(embeds,x_lengths)\n",
    "        all_hiddens, (last_hidden, last_cell) = self.lstm(embeds)      # activation function for hidden layer\n",
    "        #the first dimension of the hidden layer is 1 (we use one hidden layer uniderectional LSTMs), squeeze removes it to be \n",
    "        #able to enter the fully connected layer with dimensions [batch size, hidden size]\n",
    "        score = self.out(last_hidden.squeeze(0))           \n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(predictions, y):\n",
    "    predictions = torch.round(torch.sigmoid(predictions))\n",
    "    correct = (sum(predictions == y)).float()\n",
    "    accuracy = correct/len(predictions)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_idx = blurb_field.vocab.stoi['<pad>']\n",
    "embed_size = 100\n",
    "hidden_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmmodel = LSTMNet(len(blurb_field.vocab),embed_size,hidden_size,1, padding_idx=padding_idx, init_embedding=blurb_field.vocab.vectors)\n",
    "optimizer = optim.SGD(lstmmodel.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "lstmmodel = lstmmodel.to(device)\n",
    "loss_fn = loss_fn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, loss_fn):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for minibatch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(minibatch.blurb[0],minibatch.blurb[1]).squeeze(1)\n",
    "        loss = loss_fn(predictions, minibatch.state)\n",
    "        accuracy = calc_accuracy(predictions, minibatch.state)    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_accuracy += accuracy.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_accuracy / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, loss_fn):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for minibatch in iterator:\n",
    "            predictions = model(minibatch.blurb[0],minibatch.blurb[1]).squeeze(1)\n",
    "            loss = loss_fn(predictions, minibatch.state)\n",
    "            accuracy = calc_accuracy(predictions, minibatch.state)    \n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_accuracy += accuracy.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_accuracy / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchtext/data/field.py:321: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train), lengths\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 0.6926, Train Accuracy: 50.79%, Validation Loss: 0.6921, Validation Accuracy: 52.31%\n",
      "Total Time Passed: 0 hours, 0 minutes, 23.67 seconds\n",
      "Epoch: 02, Train Loss: 0.6920, Train Accuracy: 52.63%, Validation Loss: 0.6917, Validation Accuracy: 53.21%\n",
      "Total Time Passed: 0 hours, 0 minutes, 47.41 seconds\n",
      "Epoch: 03, Train Loss: 0.6916, Train Accuracy: 53.26%, Validation Loss: 0.6913, Validation Accuracy: 53.72%\n",
      "Total Time Passed: 0 hours, 1 minutes, 11.09 seconds\n",
      "Epoch: 04, Train Loss: 0.6913, Train Accuracy: 53.61%, Validation Loss: 0.6909, Validation Accuracy: 54.33%\n",
      "Total Time Passed: 0 hours, 1 minutes, 34.70 seconds\n",
      "Epoch: 05, Train Loss: 0.6909, Train Accuracy: 54.02%, Validation Loss: 0.6905, Validation Accuracy: 54.62%\n",
      "Total Time Passed: 0 hours, 1 minutes, 58.33 seconds\n",
      "Epoch: 06, Train Loss: 0.6905, Train Accuracy: 54.33%, Validation Loss: 0.6901, Validation Accuracy: 55.00%\n",
      "Total Time Passed: 0 hours, 2 minutes, 21.96 seconds\n",
      "Epoch: 07, Train Loss: 0.6901, Train Accuracy: 54.69%, Validation Loss: 0.6897, Validation Accuracy: 55.31%\n",
      "Total Time Passed: 0 hours, 2 minutes, 45.58 seconds\n",
      "Epoch: 08, Train Loss: 0.6898, Train Accuracy: 54.96%, Validation Loss: 0.6893, Validation Accuracy: 55.54%\n",
      "Total Time Passed: 0 hours, 3 minutes, 9.28 seconds\n",
      "Epoch: 09, Train Loss: 0.6894, Train Accuracy: 55.16%, Validation Loss: 0.6889, Validation Accuracy: 55.68%\n",
      "Total Time Passed: 0 hours, 3 minutes, 32.98 seconds\n",
      "Epoch: 10, Train Loss: 0.6890, Train Accuracy: 55.25%, Validation Loss: 0.6885, Validation Accuracy: 55.98%\n",
      "Total Time Passed: 0 hours, 3 minutes, 56.63 seconds\n",
      "Epoch: 11, Train Loss: 0.6886, Train Accuracy: 55.46%, Validation Loss: 0.6880, Validation Accuracy: 56.08%\n",
      "Total Time Passed: 0 hours, 4 minutes, 20.29 seconds\n",
      "Epoch: 12, Train Loss: 0.6882, Train Accuracy: 55.52%, Validation Loss: 0.6876, Validation Accuracy: 56.27%\n",
      "Total Time Passed: 0 hours, 4 minutes, 43.90 seconds\n",
      "Epoch: 13, Train Loss: 0.6878, Train Accuracy: 55.65%, Validation Loss: 0.6871, Validation Accuracy: 56.19%\n",
      "Total Time Passed: 0 hours, 5 minutes, 7.43 seconds\n",
      "Epoch: 14, Train Loss: 0.6874, Train Accuracy: 55.73%, Validation Loss: 0.6867, Validation Accuracy: 56.31%\n",
      "Total Time Passed: 0 hours, 5 minutes, 31.08 seconds\n",
      "Epoch: 15, Train Loss: 0.6869, Train Accuracy: 55.86%, Validation Loss: 0.6862, Validation Accuracy: 56.31%\n",
      "Total Time Passed: 0 hours, 5 minutes, 54.67 seconds\n",
      "Epoch: 16, Train Loss: 0.6865, Train Accuracy: 55.91%, Validation Loss: 0.6857, Validation Accuracy: 56.35%\n",
      "Total Time Passed: 0 hours, 6 minutes, 18.34 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-dd42030a5a40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstmmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mvalidation_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstmmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-4bf70a398ede>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblurb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblurb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-3569d34a308c>\u001b[0m in \u001b[0;36mcalc_accuracy\u001b[0;34m(predictions, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalc_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iteration over a 0-d tensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "no_epochs = 100\n",
    "#import ipdb; ipdb.set_trace() # debugging starts here\n",
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "init_time = datetime.datetime.now()\n",
    "for epoch in range(no_epochs):\n",
    "    train_loss, train_accuracy = train(lstmmodel, train_iter, optimizer, loss_fn)\n",
    "    validation_loss, validation_accuracy = evaluate(lstmmodel, validation_iter, loss_fn)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    validation_accuracies.append(validation_accuracy)\n",
    "    current_time = datetime.datetime.now()\n",
    "    total_time = (current_time-init_time).total_seconds()\n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy*100:.2f}%, ' +  \n",
    "          f'Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_accuracy*100:.2f}%')\n",
    "    print(f'Total Time Passed: {floor(total_time/3600)} hours, {floor(total_time/60)%60} minutes, {total_time%60:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = evaluate(lstmmodel, test_iter, loss_fn)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1,no_epochs+1),train_accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LSTMModelGloveTraining.txt', 'w') as f:\n",
    "    for item in train_accuracies:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LSTMModelGloveVal.txt', 'w') as f:\n",
    "    for item in validation_accuracies:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
