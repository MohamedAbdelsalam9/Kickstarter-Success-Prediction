{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import datetime\n",
    "from math import floor\n",
    "import ipdb;\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device('cuda')\n",
    "iterator_device = 0\n",
    "##uncomment for cpu\n",
    "#device = torch.device('cpu')\n",
    "#iterator_device = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add batch_first for CNN\n",
    "blurb_field = data.Field(sequential=True, use_vocab=True, lower=True, tokenize=\"spacy\", include_lengths=True, batch_first=True)\n",
    "state_field = data.LabelField(sequential=False, use_vocab=False, tensor_type=torch.FloatTensor,\n",
    "                              preprocessing=lambda x:1 if x=='successful' else 0)\n",
    "dataset = data.TabularDataset(path='df_text_eng.csv',format='csv',skip_header=True,fields=[('Unnamed: 0', None),('blurb', blurb_field),('state', state_field)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size:  150859\n",
      "Test Set Size:  32327\n",
      "Validation Set Size:  32327\n"
     ]
    }
   ],
   "source": [
    "train, test, validation = dataset.split(random_state=random.seed(seed), split_ratio=[70,15,15])\n",
    "print(\"Training Set Size: \", len(train))\n",
    "print(\"Test Set Size: \", len(test))\n",
    "print(\"Validation Set Size: \", len(validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size used:  28760\n"
     ]
    }
   ],
   "source": [
    "#words that appear less than 3 times (2 or less) will be considered unknown words with tag \"<unk>\", \n",
    "#they will have the same word embedding\n",
    "#vocabulary of the training set will only be used to emulate real world situtaions when the test set is unknown\n",
    "blurb_field.build_vocab(train,min_freq=3)\n",
    "print(\"Vocabulary size used: \",len(blurb_field.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_iter = data.BucketIterator(dataset=train, batch_size=batch_size, sort_key=lambda x:len(x.blurb),device=iterator_device,\n",
    "                                repeat=False, train=True, sort_within_batch=True)\n",
    "test_iter = data.BucketIterator(dataset=test, batch_size=batch_size, sort_key=lambda x:len(x.blurb),device=iterator_device,\n",
    "                                train=False, sort_within_batch=True)\n",
    "validation_iter = data.BucketIterator(dataset=validation, batch_size=batch_size, sort_key=lambda x:len(x.blurb),device=iterator_device,\n",
    "                                      train=False, sort_within_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, filters_num, output_size, padding_idx=None, init_embedding=None):\n",
    "        super(CNNNet, self).__init__()\n",
    "        if (init_embedding is not None):\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=padding_idx, _weight=init_embedding)    # word embedding\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=padding_idx)    # word embedding\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=1, out_channels=filters_num, kernel_size=(3,embed_size), stride=1, padding=(1,0))\n",
    "        self.conv4 = nn.Conv2d(in_channels=1, out_channels=filters_num, kernel_size=(4,embed_size), stride=1, padding=(0,0))\n",
    "        self.conv5 = nn.Conv2d(in_channels=1, out_channels=filters_num, kernel_size=(5,embed_size), stride=1, padding=(2,0))\n",
    "        self.relu = nn.ReLU();\n",
    "        \n",
    "        #put the pool in the forward function as it depends on the input sentence length, so as to choose one feature per activation map/sequence\n",
    "        #self.pool = nn.MaxPool1d(kernel_size=2, stride=1, padding=0)\n",
    "        \n",
    "        #input of filters_nup*3 as we use one feature per filter (after the pooling layer)\n",
    "        self.out = nn.Linear(filters_num*3, output_size)   # output layer (Fully Connected)\n",
    "        \n",
    "\n",
    "    def forward(self, x, x_lengths):\n",
    "        #x -> [ minibatch size, sentence length(max, smaller sentences are padded)]\n",
    "        embeds = self.embedding(x)\n",
    "        \n",
    "        #embeds -> [minibatch size, sentence length, embedding size]\n",
    "        embeds = embeds.unsqueeze(1) #make it [minibatch size, 1(number of channels), sentence length, embedding size]\n",
    "        \n",
    "        #embeds -> [minibatch size, 1, sentence length, embedding size]\n",
    "        convOut3 = self.conv3(embeds)\n",
    "        conv4pad = nn.ZeroPad2d((0,0,1,2)) ##as the conv4 needs an asymmetric padding due to even kernel size\n",
    "        convOut4 = self.conv4(conv4pad(embeds))\n",
    "        convOut5 = self.conv5(embeds)\n",
    "        \n",
    "        #convOutx -> [minibatch size, filters number for each filter size, sentence length, 1]\n",
    "        convOut = torch.cat((convOut3, convOut4, convOut5), 1)\n",
    "        \n",
    "        #convOut -> [minibatch size, total filters number, sentence length, 1]\n",
    "        convOut = self.relu(convOut)\n",
    "        \n",
    "        #convOut -> [minibatch size, total filters number, sentence length, 1]\n",
    "        convOut = convOut.squeeze(3)\n",
    "                \n",
    "        #convOut -> [minibatch size, total filters number, sentence length]\n",
    "        #kernel size of sentence length to get one feature per activation map/sequence\n",
    "        poolOut = nn.functional.max_pool1d(convOut,kernel_size = int(max(x_lengths))) \n",
    "        \n",
    "        #poolOut -> [minibatch size, total filters number, 1]\n",
    "        #squeeze makes poolOut [minibatch size, total filters number] before entering the fully connected layer\n",
    "        score = self.out(poolOut.squeeze(2))     \n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(predictions, y):\n",
    "    predictions = torch.round(torch.sigmoid(predictions))\n",
    "    correct = (sum(predictions == y)).float()\n",
    "    accuracy = correct/len(predictions)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_idx = blurb_field.vocab.stoi['<pad>']\n",
    "embed_size = 100\n",
    "filters_num = 100 #filters number per kernel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnmodel = CNNNet(len(blurb_field.vocab),embed_size,filters_num,1, padding_idx=padding_idx)\n",
    "optimizer = optim.SGD(cnnmodel.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "cnnmodel = cnnmodel.to(device)\n",
    "loss_fn = loss_fn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, loss_fn):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for minibatch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(minibatch.blurb[0],minibatch.blurb[1]).squeeze(1)\n",
    "        loss = loss_fn(predictions, minibatch.state)\n",
    "        accuracy = calc_accuracy(predictions, minibatch.state)    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_accuracy += accuracy.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_accuracy / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, loss_fn):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for minibatch in iterator:\n",
    "            predictions = model(minibatch.blurb[0],minibatch.blurb[1]).squeeze(1)\n",
    "            loss = loss_fn(predictions, minibatch.state)\n",
    "            accuracy = calc_accuracy(predictions, minibatch.state)    \n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_accuracy += accuracy.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_accuracy / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchtext/data/field.py:321: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train), lengths\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 0.6925, Train Accuracy: 52.30%, Validation Loss: 0.6876, Validation Accuracy: 54.28%\n",
      "Total Time Passed: 0 hours, 4 minutes, 12.94 seconds\n",
      "Epoch: 02, Train Loss: 0.6853, Train Accuracy: 55.14%, Validation Loss: 0.6821, Validation Accuracy: 56.06%\n",
      "Total Time Passed: 0 hours, 8 minutes, 26.03 seconds\n",
      "Epoch: 03, Train Loss: 0.6798, Train Accuracy: 56.74%, Validation Loss: 0.6782, Validation Accuracy: 56.98%\n",
      "Total Time Passed: 0 hours, 12 minutes, 38.90 seconds\n",
      "Epoch: 04, Train Loss: 0.6755, Train Accuracy: 57.67%, Validation Loss: 0.6746, Validation Accuracy: 57.91%\n",
      "Total Time Passed: 0 hours, 16 minutes, 51.67 seconds\n",
      "Epoch: 05, Train Loss: 0.6717, Train Accuracy: 58.42%, Validation Loss: 0.6717, Validation Accuracy: 58.33%\n",
      "Total Time Passed: 0 hours, 21 minutes, 4.16 seconds\n",
      "Epoch: 06, Train Loss: 0.6682, Train Accuracy: 58.96%, Validation Loss: 0.6692, Validation Accuracy: 58.67%\n",
      "Total Time Passed: 0 hours, 25 minutes, 16.85 seconds\n",
      "Epoch: 07, Train Loss: 0.6650, Train Accuracy: 59.49%, Validation Loss: 0.6671, Validation Accuracy: 59.08%\n",
      "Total Time Passed: 0 hours, 29 minutes, 29.33 seconds\n",
      "Epoch: 08, Train Loss: 0.6621, Train Accuracy: 59.91%, Validation Loss: 0.6648, Validation Accuracy: 59.49%\n",
      "Total Time Passed: 0 hours, 33 minutes, 41.87 seconds\n",
      "Epoch: 09, Train Loss: 0.6591, Train Accuracy: 60.37%, Validation Loss: 0.6628, Validation Accuracy: 59.73%\n",
      "Total Time Passed: 0 hours, 37 minutes, 54.41 seconds\n",
      "Epoch: 10, Train Loss: 0.6563, Train Accuracy: 60.83%, Validation Loss: 0.6625, Validation Accuracy: 59.58%\n",
      "Total Time Passed: 0 hours, 42 minutes, 7.01 seconds\n"
     ]
    }
   ],
   "source": [
    "no_epochs = 10\n",
    "#import ipdb; ipdb.set_trace() # debugging starts here\n",
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "init_time = datetime.datetime.now()\n",
    "for epoch in range(no_epochs):\n",
    "    train_loss, train_accuracy = train(cnnmodel, train_iter, optimizer, loss_fn)\n",
    "    validation_loss, validation_accuracy = evaluate(cnnmodel, validation_iter, loss_fn)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    validation_accuracies.append(validation_accuracy)\n",
    "    current_time = datetime.datetime.now()\n",
    "    total_time = (current_time-init_time).total_seconds()\n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy*100:.2f}%, ' +  \n",
    "          f'Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_accuracy*100:.2f}%')\n",
    "    print(f'Total Time Passed: {floor(total_time/3600)} hours, {floor(total_time/60)%60} minutes, {total_time%60:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchtext/data/field.py:321: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train), lengths\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6603, Test Accuracy: 60.33%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = evaluate(cnnmodel, test_iter, loss_fn)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1,no_epochs+1),train_accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('CNNModelTraining.txt', 'w') as f:\n",
    "    for item in train_accuracies:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('CNNModelVal.txt', 'w') as f:\n",
    "    for item in validation_accuracies:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
